{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluating eye tracker calibration models with the Akaike information criterion",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "grWwcfr9RMjw",
        "qwGpt11e6WAL",
        "4lfCYgEVNWvi"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfernandoghe/Evaluating-eye-tracker-calibration-models-with-the-Akaike-information-criterion/blob/master/Evaluating_eye_tracker_calibration_models_with_the_Akaike_information_criterion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "grWwcfr9RMjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download dataset\n",
        "Options: Background luminance was varied systematically from black to white in 7 steps (0, 12.5, 25, 37, 50, 75 and\n",
        "100% of the available screen luminance). Individual information per eye, 3 repetitions per user."
      ]
    },
    {
      "metadata": {
        "id": "-8J3CafVs5mD",
        "colab_type": "code",
        "outputId": "1adfa1c0-85dc-4344-d472-4a0cc90e32b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir PreProcessed\n",
        "!mkdir OutputTechniques\n",
        "!mkdir OutputMetrics\n",
        "!cd PreProcessed && git clone https://github.com/jfernandoghe/Dataset.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Dataset'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 12 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwGpt11e6WAL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Obtain latest modified RANSAC Regressor from public repository\n",
        "Modified version of original sklearn RANSAC Regressor"
      ]
    },
    {
      "metadata": {
        "id": "BDiLBOhXL9qK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "c0a9c921-bd3d-456e-cda7-1b50f2da2d6d"
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  !rm -r /content/RANSACRegressor2\n",
        "  !git clone https://github.com/jfernandoghe/RANSACRegressor2.git   \n",
        "except:\n",
        "  !git clone https://github.com/jfernandoghe/RANSACRegressor2.git        "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/RANSACRegressor2': No such file or directory\n",
            "Cloning into 'RANSACRegressor2'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 15 (delta 3), reused 8 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Bc8AEYsk7AQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import numpy.linalg as lalg\n",
        "# from numpy import where\n",
        "# from itertools import compress\n",
        "# import io\n",
        "# import re\n",
        "# import random\n",
        "# from itertools import combinations, chain\n",
        "# import math\n",
        "# from math import log\n",
        "# import itertools\n",
        "# import csv\n",
        "# import statistics\n",
        "# ####\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.linear_model.ransac import _dynamic_max_trials\n",
        "# from sklearn.utils import check_random_state, check_array, check_consistent_length\n",
        "# from sklearn.utils.validation import has_fit_parameter\n",
        "# from sklearn.utils.random import sample_without_replacement\n",
        "\n",
        "\n",
        "\n",
        "# class RANSACRegressor2(linear_model.RANSACRegressor):\n",
        "  \n",
        "#   def fitSegm(self, X, y, segmList, sample_weight=None):\n",
        "    \n",
        "#     merged = list(itertools.chain.from_iterable(segmList))\n",
        "  \n",
        "    \n",
        "#     X = check_array(X, accept_sparse='csr')\n",
        "#     y = check_array(y, ensure_2d=False)\n",
        "#     check_consistent_length(X, y)\n",
        "\n",
        "    \n",
        "    \n",
        "#     if self.base_estimator is not None:\n",
        "#         base_estimator = clone(self.base_estimator)\n",
        "#     else:\n",
        "#         base_estimator = LinearRegression()\n",
        "\n",
        "#     if self.min_samples is None:\n",
        "#         # assume linear model by default\n",
        "#         min_samples = X.shape[1] + 1 if len(segmList)<2 else X.shape[1]      #MINIMUM SAMPLES   \n",
        "#     elif 0 < self.min_samples < 1:\n",
        "#         min_samples = np.ceil(self.min_samples * X.shape[0])\n",
        "#     elif self.min_samples >= 1:\n",
        "#         if self.min_samples % 1 != 0:\n",
        "#             raise ValueError(\"Absolute number of samples must be an \"\n",
        "#                              \"integer value.\")\n",
        "#         min_samples = self.min_samples\n",
        "#     else:\n",
        "#         raise ValueError(\"Value for `min_samples` must be scalar and \"\n",
        "#                          \"positive.\")\n",
        "#     if min_samples > X.shape[0]:\n",
        "#         raise ValueError(\"`min_samples` may not be larger than number \"\n",
        "#                          \"of samples: n_samples = %d.\" % (X.shape[0]))\n",
        "\n",
        "#     if self.stop_probability < 0 or self.stop_probability > 1:\n",
        "#         raise ValueError(\"`stop_probability` must be in range [0, 1].\")\n",
        "\n",
        "#     if self.residual_threshold is None:\n",
        "#         # MAD (median absolute deviation)\n",
        "#         residual_threshold = np.median(np.abs(y - np.median(y)))\n",
        "#     else:\n",
        "#         residual_threshold = self.residual_threshold\n",
        "\n",
        "#     if self.loss == \"absolute_loss\":\n",
        "#         if y.ndim == 1:\n",
        "#             loss_function = lambda y_true, y_pred: np.abs(y_true - y_pred)\n",
        "#         else:\n",
        "#             loss_function = lambda \\\n",
        "#                 y_true, y_pred: np.sum(np.abs(y_true - y_pred), axis=1)\n",
        "\n",
        "#     elif self.loss == \"squared_loss\":\n",
        "#         if y.ndim == 1:\n",
        "#             loss_function = lambda y_true, y_pred: (y_true - y_pred) ** 2\n",
        "#         else:\n",
        "#             loss_function = lambda \\\n",
        "#                 y_true, y_pred: np.sum((y_true - y_pred) ** 2, axis=1)\n",
        "\n",
        "#     elif callable(self.loss):\n",
        "#         loss_function = self.loss\n",
        "\n",
        "#     else:\n",
        "#         raise ValueError(\n",
        "#             \"loss should be 'absolute_loss', 'squared_loss' or a callable.\"\n",
        "#             \"Got %s. \" % self.loss)\n",
        "\n",
        "\n",
        "#     random_state = check_random_state(self.random_state)\n",
        "\n",
        "#     try:  # Not all estimator accept a random_state\n",
        "#         base_estimator.set_params(random_state=random_state)\n",
        "#     except ValueError:\n",
        "#         pass\n",
        "\n",
        "#     estimator_fit_has_sample_weight = has_fit_parameter(base_estimator,\n",
        "#                                                         \"sample_weight\")\n",
        "#     estimator_name = type(base_estimator).__name__\n",
        "#     if (sample_weight is not None and not\n",
        "#             estimator_fit_has_sample_weight):\n",
        "#         raise ValueError(\"%s does not support sample_weight. Samples\"\n",
        "#                          \" weights are only used for the calibration\"\n",
        "#                          \" itself.\" % estimator_name)\n",
        "#     if sample_weight is not None:\n",
        "#         sample_weight = np.asarray(sample_weight)\n",
        "\n",
        "#     n_inliers_best = 1\n",
        "#     score_best = -np.inf\n",
        "#     inlier_mask_best = None\n",
        "#     X_inlier_best = None\n",
        "#     y_inlier_best = None\n",
        "#     aicc_ = None\n",
        "#     self.n_skips_no_inliers_ = 0\n",
        "#     self.n_skips_invalid_data_ = 0\n",
        "#     self.n_skips_invalid_model_ = 0\n",
        "\n",
        "    \n",
        "       \n",
        "#     # Generate a list of indices for each segment\n",
        "#     size_sl = [len(s)-1 for s in segmList]\n",
        "#     n_segments = len(size_sl)\n",
        "    \n",
        "    \n",
        "#     # number of data samples\n",
        "#     n_samples = X.shape[0] \n",
        "#     sample_idxs = np.arange(n_samples)\n",
        "\n",
        "#     n_samples, _ = X.shape\n",
        "    \n",
        "    \n",
        "#     self.n_trials_ = 0\n",
        "#     max_trials = self.max_trials\n",
        "    \n",
        " \n",
        "    \n",
        "    \n",
        "#     while self.n_trials_ < max_trials:\n",
        "#         self.n_trials_ += 1\n",
        "#         if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\n",
        "#                 self.n_skips_invalid_model_) > self.max_skips:\n",
        "#             break\n",
        "       \n",
        "\n",
        "\n",
        "        \n",
        "#         # choose random sample set\n",
        "#         ## antes:\n",
        "#         ## subset_idxs = sample_without_replacement(n_samples, min_samples, random_state=random_state)\n",
        "#         ## ahora:\n",
        "#         subset_idx_entries = sample_without_replacement(n_segments, min_samples,\n",
        "#                                                  random_state=random_state)\n",
        "        \n",
        "        \n",
        "        \n",
        "#         subset_idxs = np.asarray([segmList[ss][random.randint(0, size_sl[ss])] \\\n",
        "#                        for ss in subset_idx_entries])\n",
        "   \n",
        "    \n",
        "        \n",
        "        \n",
        "#         X_subset = X[subset_idxs]\n",
        "#         y_subset = y[subset_idxs]\n",
        "\n",
        "#         # check if random sample set is valid\n",
        "#         if (self.is_data_valid is not None\n",
        "#                 and not self.is_data_valid(X_subset, y_subset)):\n",
        "#             self.n_skips_invalid_data_ += 1\n",
        "#             continue        \n",
        "\n",
        "#         # fit model for current random sample set\n",
        "#         if sample_weight is None:\n",
        "#           base_estimator.fit(X_subset, y_subset)\n",
        "#         else:\n",
        "#           base_estimator.fit(X_subset, y_subset, sample_weight=sample_weight[subset_idxs])\n",
        "\n",
        "#         # check if estimated model is valid\n",
        "#         if (self.is_model_valid is not None and not\n",
        "#                 self.is_model_valid(base_estimator, X_subset, y_subset)):\n",
        "#             self.n_skips_invalid_model_ += 1\n",
        "#             continue\n",
        "            \n",
        "#         # check if estimated model is valid (ii)\n",
        "#         y_pred_subset = base_estimator.predict(X_subset)\n",
        "#         residuals_ii = loss_function(y_subset, y_pred_subset)\n",
        "#         inlier_mask_subset_ii = residuals_ii < residual_threshold\n",
        "        \n",
        "        \n",
        "#         if np.sum(inlier_mask_subset_ii)< X.shape[1]:\n",
        "#           self.n_skips_invalid_model_ += 1\n",
        "#           continue      \n",
        "        \n",
        "      \n",
        "            \n",
        "          \n",
        "          \n",
        "          \n",
        "#         # residuals of all data for current random sample model\n",
        "#         y_pred = base_estimator.predict(X[merged])\n",
        "\n",
        "\n",
        "#         residuals_subset = loss_function(y[merged], y_pred)\n",
        "#         # classify data into inliers and outliers\n",
        "#         inlier_mask_subset = residuals_subset < residual_threshold   \n",
        "\n",
        "#         n_inliers_subset = np.sum(inlier_mask_subset)\n",
        "        \n",
        "#         #check that the all points in sample are inliers\n",
        "#         if n_inliers_subset < min_samples:\n",
        "#             continue\n",
        "        \n",
        "#         # less inliers -> skip current random sample\n",
        "#         if n_inliers_subset < n_inliers_best:\n",
        "#             self.n_skips_no_inliers_ += 1\n",
        "#             continue\n",
        "\n",
        "            \n",
        "#         # extract inlier data set        \n",
        "#         inlier_idxs_subset = list(compress(merged, inlier_mask_subset))\n",
        "        \n",
        "#         X_inlier_subset = X[inlier_idxs_subset]\n",
        "#         y_inlier_subset = y[inlier_idxs_subset]\n",
        "        \n",
        "\n",
        "#         # score of inlier data set\n",
        "#         score_subset = base_estimator.score(X_inlier_subset,\n",
        "#                                             y_inlier_subset)\n",
        "\n",
        "\n",
        "#         # same number of inliers but worse score -> skip current random\n",
        "#         # sample\n",
        "#         if (n_inliers_subset == n_inliers_best\n",
        "#                 and score_subset <= score_best):\n",
        "#             continue\n",
        "\n",
        "            \n",
        "    \n",
        "#         # save current random sample as best sample\n",
        "#         n_inliers_best = n_inliers_subset\n",
        "#         score_best = score_subset\n",
        "#         inlier_mask_best = inlier_mask_subset\n",
        "#         X_inlier_best = X_inlier_subset\n",
        "#         y_inlier_best = y_inlier_subset\n",
        "        \n",
        "\n",
        "#         max_trials = min(\n",
        "#             max_trials,\n",
        "#             _dynamic_max_trials(n_inliers_best, n_samples,\n",
        "#                                 min_samples, self.stop_probability))\n",
        "\n",
        "\n",
        "#         # break if sufficient number of inliers or score is reached\n",
        "#         if n_inliers_best >= self.stop_n_inliers or \\\n",
        "#                         score_best >= self.stop_score:\n",
        "#             break\n",
        "\n",
        "#   # if none of the iterations met the required criteria\n",
        "#     if inlier_mask_best is None:\n",
        "#         base_estimator.coef_=-999\n",
        "#         if ((self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\n",
        "#                 self.n_skips_invalid_model_) > self.max_skips):\n",
        "#             raise ValueError(\n",
        "#                 \"RANSAC skipped more iterations than `max_skips` without\"\n",
        "#                 \" finding a valid consensus set. Iterations were skipped\"\n",
        "#                 \" because each randomly chosen sub-sample failed the\"\n",
        "#                 \" passing criteria. See estimator attributes for\"\n",
        "#                 \" diagnostics (n_skips*).\")\n",
        "#         else:\n",
        "#             raise ValueError(\n",
        "#                 \"RANSAC could not find a valid consensus set. All\"\n",
        "#                 \" `max_trials` iterations were skipped because each\"\n",
        "#                 \" randomly chosen sub-sample failed the passing criteria.\"\n",
        "#                 \" See estimator attributes for diagnostics (n_skips*).\")\n",
        "#     else:\n",
        "#         if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +\n",
        "#                 self.n_skips_invalid_model_) > self.max_skips:\n",
        "#             warnings.warn(\"RANSAC found a valid consensus set but exited\"\n",
        "#                           \" early due to skipping more iterations than\"\n",
        "#                           \" `max_skips`. See estimator attributes for\"\n",
        "#                           \" diagnostics (n_skips*).\",\n",
        "#                           ConvergenceWarning)\n",
        "#     # estimate final model using all inliers\n",
        "#         base_estimator.fit(X_inlier_best, y_inlier_best)    \n",
        "#         self.estimator_ = base_estimator\n",
        "#         self.inlier_mask_ = inlier_mask_best\n",
        "#         return self\n",
        "# ##############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lfCYgEVNWvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define and import modules, classes, functions and packages\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0pWa6Y8hHSc_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#@title\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy.linalg as lalg\n",
        "from numpy import where\n",
        "import io\n",
        "import re\n",
        "import random\n",
        "from itertools import combinations, chain\n",
        "import math\n",
        "from math import log\n",
        "import itertools\n",
        "import csv\n",
        "import statistics\n",
        "import scipy\n",
        "\n",
        "def powerset(input_set, terms):\n",
        "    print('terms ', terms)\n",
        "    result = []\n",
        "    for size in range(terms + 1):\n",
        "        result += combinations(input_set, size)\n",
        "    return result\n",
        "      \n",
        "class TipoPrueba:\n",
        "  def __init__(self, pant, med, grouping): \n",
        "    self.pant = pant\n",
        "    self.med = med\n",
        "    self.grouping = grouping\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"Ajustar columnas de mediciones %s a las de pantalla %i con agrup %i\" \\\n",
        "  % (self.med, self.pant, self.grouping)\n",
        "\n",
        "def modeval_ransac(A,b):\n",
        "  parnum = A.ndim\n",
        "  numite = 100\n",
        "  AIC = 0\n",
        "  AICc = 0\n",
        "  MSE = 0\n",
        "  return AIC, AICc, MSE\n",
        "\n",
        "def c_pow(val, n):\n",
        "  return [ val**j  for j in range(0,n+1)]\n",
        "          \n",
        "          \n",
        "def allTerms(valXY, m, n):\n",
        "  v_m = c_pow(valXY[0], m)\n",
        "  v_n = c_pow(valXY[1], n)\n",
        "  mmax = max(m,n)\n",
        "  vals = [v_m[a]*v_n[b] for a in range(m+1) for b in range(n+1) if a+b<= mmax]\n",
        "  return vals\n",
        "\n",
        "def allSymbTerms(m, n):\n",
        "  sym_m = [ f'x^{j}' if j>1 else ('x' if j == 1 else '')  for j in range(0,m+1)]\n",
        "  sym_n = [ f'y^{j}' if j>1 else ('y' if j == 1 else '')  for j in range(0,n+1)]\n",
        "  mmax = max(m,n)\n",
        "  symb = [sym_m[a]+sym_n[b] if len(sym_m[a]+sym_n[b])>0 else '1' for a in range(m+1) for b in range(n+1) if a+b<= mmax] \n",
        "  return symb\n",
        "\n",
        "################\n",
        "#  Convert pixels to angle (in degrees)\n",
        "#     parameters: \n",
        "#     v1, pixel position\n",
        "#     smm, monitor size (mm)\n",
        "#     d,   eye distance\n",
        "def dst2Degree(v1, d, smm, spx):\n",
        "  # distance to screen center (in pixels)\n",
        "  v1c = v1 - spx/2 \n",
        "  \n",
        "  # convert pixel position in pixels to mm\n",
        "  v1mm = (v1c*smm)/spx\n",
        "  \n",
        "  \n",
        "  # calculate and return angle b \n",
        "  #      v1mm\n",
        "  #     ____ \n",
        "  #     |  /\n",
        "  #  d  |b/\n",
        "  #     |/ \n",
        "  return math.degrees( math.atan2(v1mm, d) )\n",
        "\n",
        "\n",
        "\n",
        "def removeSpurious(a, perc):\n",
        "  num = int(round(len(a) * perc/100.0))\n",
        "  for i in range(num):\n",
        "    a = np.delete(a, a.argmax()) \n",
        "  return a\n",
        "\n",
        "\n",
        "def removeSpuriousN(a, num):\n",
        "  for i in range(num):    \n",
        "    a = np.delete(a, a.argmax()) \n",
        "  return a\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LqSOv6hkCjt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Main algorithm"
      ]
    },
    {
      "metadata": {
        "id": "04y6GN6JHaX7",
        "colab_type": "code",
        "outputId": "1e3e115f-dfae-4650-90b2-6585a9615671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import glob\n",
        "from RANSACRegressor2 import RANSACRegressor2\n",
        "# Local Variables\n",
        "max_m = 3\n",
        "max_n = 2\n",
        "mn = (max_m+1)*(max_n+1)\n",
        "max_degreeterm = 2\n",
        "t = TipoPrueba(1, (3,4), 5) # X configuration \n",
        "# t = TipoPrueba(2, (3,4), 5) # Y configuration \n",
        "percData = 8\n",
        "coor = [ {228,370,512,654,796,100,242,384,526,668}, {470,655,840,1025,1210,155,340,525,710,895} ] #Coordinates of calibrations points\n",
        "res = [ [1600,1200], [1920, 1080] ] #Resolution in pixeles\n",
        "Smm = [ [487, 274], [508, 406.4] ] #Width and Height in mm\n",
        "\n",
        "\n",
        "path = \"/content/PreProcessed/Dataset/Data/*.csv\" \n",
        "for fname in glob.glob(path):\n",
        "  df = pd.read_csv(fname, sep=';', header=None, names=[\"Participant\", \"Xp\", \"Yp\", \"x\", \"y\", \"CalPoint\", \"tStamp\", \"d_eye\"])\n",
        "\n",
        "  # CSV Read\n",
        "  # Header: Participant, x', y', x, y, CalPoint :\n",
        "  #df = pd.read_csv('/content/PupilDynamics_Right.csv', sep=';', header=None, names=[\"Participant\", \"Xp\", \"Yp\", \"x\", \"y\", \"CalPoint\", \"tStamp\", \"dRight\", \"dLeft\"])\n",
        "  #df = pd.read_csv('/content/drive/My Drive/06_Public/03_Datasets/G_Right.csv', sep=';', header=None, names=[\"Participant\", \"Xp\", \"Yp\", \"x\", \"y\", \"CalPoint\", \"tStamp\", \"dLeft\", \"dRight\"])\n",
        "#   df = pd.read_csv('/content/PreProcessed/PupilDynamics_Right_l1.csv', sep=';', header=None, names=[\"Participant\", \"Xp\", \"Yp\", \"x\", \"y\", \"CalPoint\", \"tStamp\", \"d_eye\"])\n",
        "  df = df[df.x != 0]\n",
        "  df = df[df.y != 0]\n",
        "\n",
        "  df = df[df.Xp != 0]\n",
        "  df = df[df.Yp != 0]\n",
        "  # df = df[df.dLeft != 0]\n",
        "  # df = df[df.dRight != 0]\n",
        "  df = df[df.d_eye != 0]\n",
        "  df = df.fillna(0)\n",
        "  df = df.values\n",
        "  M = np.array(df)\n",
        "\n",
        "\n",
        "\n",
        "  #subsampling\n",
        "  M = M[::20, : ]\n",
        "\n",
        "\n",
        "\n",
        "  users = np.unique(M[:,0])\n",
        "  # users = [5]\n",
        "  #print(users)\n",
        "  #users = np.concatenate( [users[:14],users[16:]] )\n",
        "\n",
        "\n",
        "  #print(\"Terms --->\", allTermStr)\n",
        "  print('___________________________________________')\n",
        "  for part in users:\n",
        "    print(\"\")\n",
        "    print(\"************************************************************************************\")\n",
        "    print ('Participant ', part)\n",
        "    print(\"************************************************************************************\")\n",
        "    print(\"\")\n",
        "\n",
        "    mPart = M[where(M[:, 0] == part)] # Participant' data\n",
        "    bb = set(mPart[:,1])     #Ejemplo de X, índice [0] \n",
        "    \n",
        "    \n",
        "    \n",
        "#***************************************\n",
        "    if bool(bb & coor[0]):   #Check screentype\n",
        "      mon = 0\n",
        "    elif bool(bb & coor[1]):\n",
        "      mon = 1    \n",
        "#***************************************\n",
        "    \n",
        "    \n",
        "    b = mPart[:, t.pant]\n",
        "    d = mPart[:, t.med]   # XY data\n",
        "\n",
        "    group = mPart[:, t.grouping] \n",
        "    segmList = [where(group==i)[0] for i in range(1,26)]\n",
        "    allTermStr = allSymbTerms (max_m, max_n)\n",
        "    s1 = np.asarray( [allTerms(xy, max_m, max_n) for xy in d] )\n",
        "\n",
        "\n",
        "    setA = {1, 17, 24, 13, 10, 19, 7, 21, 3} #Training set\n",
        "    setB = {i for i in range (25) } #Total set\n",
        "    setC = setB - setA #Validation set\n",
        "\n",
        "    if ( len(set( np.unique(mPart[:,5]).flatten().astype(int)-1 ).symmetric_difference(setB))==0 ):\n",
        "\n",
        "\n",
        "      segmLi1 = [segmList[i] for i in setA] # training-measurements\n",
        "      merged1 = list(itertools.chain.from_iterable(segmLi1))\n",
        "      segmLi2 = [segmList[i] for i in setC]   # test-measurements\n",
        "\n",
        "\n",
        "\n",
        "      # common approach\n",
        "      # (a) remove the first 10 percent of measurements (saccadic jumps SJ)\n",
        "      segmLstNSJ = [ lst[math.ceil(len(lst)/percData):] for lst in segmLi1] # list without saccadic_jumps\n",
        "      # (b) pick the median value\n",
        "      b_mm = [ statistics.median_high(b[lst]) for lst in segmLstNSJ]\n",
        "      d_mm = [ [statistics.median_high(d[lst,0]),statistics.median_high(d[lst,1])] for lst in segmLstNSJ] # XY data\n",
        "      s1_mm = np.asarray( [allTerms(xy, max_m, max_n) for xy in d_mm] )\n",
        "\n",
        "\n",
        "      #models =  powerset([i for i in range(s1.shape[1])], len(segmLi1))[1:]\n",
        "\n",
        "      #Terms   0    1     2     3    4       5      6      7       8\n",
        "      #Terms ['1', 'y', 'y^2', 'x', 'xy', 'xy^2', 'x^2', 'x^2y', 'x^3']\n",
        "      #models =[[3,0]]\n",
        "      #models = [[3,0], [4,0], [5,0], [6,0], [7,0], [8,0] , [4, 3, 0]]\n",
        "      \n",
        "      # X-Models  0        1          2          3          4          5            6             7            8             9             10            11             12\n",
        "      models = [[3,0], [3, 1, 0], [4, 3, 0], [6, 3, 0], [8, 3, 0], [6, 3, 0], [4, 3, 1, 0], [8, 6, 3, 0], [1, 6, 3, 0], [4, 6, 3, 0], [1, 8, 3, 0], [2, 8, 3, 0], [4, 8, 3, 0] ]\n",
        "\n",
        "      # Y-Models\n",
        "      # models = [[1, 0], [3, 1, 0], [4, 1, 0], [2, 1, 0], [1, 6, 0], [1, 4, 3, 0],[2, 1, 3, 0], [5, 3, 1, 0], [4, 1, 6, 0], [7, 6, 1, 0]] \n",
        "\n",
        "\n",
        "      selStr = [\"AIC\", \"AICc\", \"KIC\", \"KICc\", \"AICF\", \"KICc\", \"RAIC\"] \n",
        "\n",
        "\n",
        "      # selectors explored: AIC, AICc, AKIC, AKICc, AICF, KICc, RAIC\n",
        "      errR2_4all = np.zeros((len(models), len(setA)))\n",
        "\n",
        "#       rThreshold = 40\n",
        "      rThreshold = (res[mon][t.grouping-5] *  )/Smm[mon][t.grouping-5]\n",
        "  \n",
        "      modelNum =0\n",
        "      errorsEv = np.zeros([len(setC)])\n",
        "      for model in models: \n",
        "          maxBits = len(allTermStr)\n",
        "          if len(model)>0: # and len(model)<=max_degreeterm: # Select Models that accomplish certain parameter conditions (e.g., up to 5 parameters)\n",
        "            A = s1[:,model]\n",
        "\n",
        "\n",
        "            ######### training phase\n",
        "            ########################\n",
        "            allOk = True\n",
        "            try:\n",
        "              #  Fit Simple Linear model (1)\n",
        "              Am = A[merged1, :]\n",
        "              bm = b[merged1] \n",
        "              soln =  lalg.lstsq(Am, bm, rcond=None)\n",
        "              theta = soln[0]\n",
        "              residues = soln[1]\n",
        "\n",
        "\n",
        "              # Fit linear model with point selection (median)  (2)\n",
        "              A_mm = s1_mm[:,model]\n",
        "              soln_mm =  lalg.lstsq(A_mm, b_mm, rcond=None)\n",
        "              theta_mm = soln_mm[0]\n",
        "              residues_mm = soln_mm[1]\n",
        "\n",
        "\n",
        "              # Robustly fit linear model with RANSAC algorithm (Robust Linear Model) (3)\n",
        "              ransac = RANSACRegressor2(residual_threshold=rThreshold)\n",
        "              ransac.fitSegm(A, b, segmLi1)\n",
        "\n",
        "              pos = 0\n",
        "              for li in segmLi1:\n",
        "                Avalid = np.array( [A[i, :] for i in li] )\n",
        "                bvalid = np.array( [b[i] for i in li] )\n",
        "\n",
        "                errR = ransac.predict(Avalid)-bvalid\n",
        "                errR2 = np.multiply(errR, errR)\n",
        "                errR2_4all[modelNum][pos] =  np.percentile(errR2, 50)\n",
        "                pos += 1\n",
        "\n",
        "\n",
        "            except ValueError as e:\n",
        "              allOk = False\n",
        "              print('errX', e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #end_try  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ######### evaluation phase\n",
        "            ########################\n",
        "            pos1 = 0\n",
        "            betaR = np.zeros([len(setC)])\n",
        "            betaS = np.zeros([len(setC)])\n",
        "            betaC = np.zeros([len(setC)])\n",
        "\n",
        "\n",
        "            # Calculate and save errors\n",
        "\n",
        "            for li in segmLi2:\n",
        "              Atest = np.array( [A[i, :] for i in li] )\n",
        "              btest = np.array( [b[i] for i in li] )\n",
        "              disL = np.array( [(mPart[i, 7])/10 for i in li] ) \n",
        "\n",
        "              # estimation from a simple model (1)\n",
        "              estimated =  np.matmul(Atest,theta)\n",
        "\n",
        "              # estimation from saccadic_and_median filter (2)\n",
        "              estimated_mm =  np.matmul(Atest,theta_mm)\n",
        "\n",
        "              # estimation from Ransac (2)\n",
        "              if allOk:    \n",
        "                estimated_rr = ransac.predict(Atest) \n",
        "              else:\n",
        "                estimated_rr = 9999              \n",
        "\n",
        "#               # error in DEGREES\n",
        "#               if bool(bb & coor[0]):\n",
        "#                 mon = 0\n",
        "#               elif bool(bb & coor[1]):\n",
        "#                 mon = 1\n",
        "\n",
        "\n",
        "              ang  = [dst2Degree(btest[b], disL[b], Smm[mon][t.grouping-5], res[mon][t.grouping-5]) for b in range(len(disL))]\n",
        "              ang = np.array(ang)\n",
        "              ranAng = [dst2Degree(estimated_rr[b], disL[b], Smm[mon][t.grouping-5], res[mon][t.grouping-5]) for b in range(len(disL))]\n",
        "              comAng = [dst2Degree(estimated[b],    disL[b], Smm[mon][t.grouping-5], res[mon][t.grouping-5]) for b in range(len(disL))]\n",
        "              simAng = [dst2Degree(estimated_mm[b], disL[b], Smm[mon][t.grouping-5], res[mon][t.grouping-5]) for b in range(len(disL))]\n",
        "\n",
        "              betaR[pos1] = np.median(abs(np.array(ranAng)- ang))\n",
        "              betaS[pos1] = np.median(abs(np.array(comAng)- ang))\n",
        "              betaC[pos1] = np.median(abs(np.array(simAng)- ang))\n",
        "\n",
        "\n",
        "\n",
        "              pos1 += 1\n",
        "\n",
        "            #end_for\n",
        "\n",
        "\n",
        "\n",
        "            usedTerms = [allTermStr[o] for o in model]\n",
        "\n",
        "            mR = np.median(betaR)            \n",
        "            mS = np.median(betaS)\n",
        "            mC = np.median(betaC)\n",
        "\n",
        "\n",
        "\n",
        "            print(\"modelNum\", modelNum )\n",
        "            print(usedTerms)\n",
        "\n",
        "            print(f'Ransac:  {mR}')\n",
        "            print(\"Simple: \", mS )\n",
        "            print(\"Common: \", mC )\n",
        "            print(\"-----\")\n",
        "            errorsEv[modelNum] = mR\n",
        "\n",
        "            with open('/content/OutputTechniques/'+fname[-10:], 'a') as csvFile:\n",
        "              writer = csv.writer(csvFile, delimiter=';')\n",
        "\n",
        "              # participant, model, strategy, modelName, value\n",
        "              writer.writerow([part, modelNum, 0, usedTerms, mR])\n",
        "              writer.writerow([part, modelNum, 1, usedTerms, mS])\n",
        "              writer.writerow([part, modelNum, 2, usedTerms, mC])\n",
        "            csvFile.close()      \n",
        "            modelNum += 1\n",
        "\n",
        "\n",
        "      #print(errR2_4all)\n",
        "      ################### simulate selection\n",
        "      inAll = errR2_4all > rThreshold*rThreshold*1.2*1.2   \n",
        "      toDelete = min(inAll.sum(axis = 1))\n",
        "      #print(\"points to delete\", toDelete)\n",
        "      bestSelectorVal = np.ones(7) *  np.inf\n",
        "      selectorError = np.zeros(7)    \n",
        "      \n",
        "      modelNum = 0\n",
        "      for model in models: # evaluate each model      \n",
        "        k = len(model)\n",
        "        n = len(setA)-toDelete\n",
        "        if 2*k <= n:\n",
        "          rssI = removeSpuriousN(errR2_4all[modelNum], toDelete)\n",
        "          #print(\"rssI\", rssI)\n",
        "          sumRsc = np.sum(rssI)\n",
        "          c = rThreshold * rThreshold\n",
        "          sigmaHat = sumRsc/n\n",
        "          errRnorm =  rssI/sigmaHat\n",
        "          rhoError = [(r*r)/2 if abs(r)<=c else c*r*r- c*c/2  for r in errRnorm]\n",
        "\n",
        "          AIC = 2*(k) + n*math.log(sumRsc/n)\n",
        "          AICc = AIC + (2*k**2 + 2*k )/ (n - k - 1) if n>k+1 else -999\n",
        "          KIC = n*math.log(sumRsc/n) + 3*(k+1)\n",
        "          KICc = n*math.log(sumRsc/n) + (2*(k+1)*n)/(n-k-2) - n*scipy.special.digamma((n-k)/2)*((n-k)/2) + n* math.log(n/2) if n>k+2 else -999 # definir PSI\n",
        "          AKICc = n*math.log(sumRsc/n) + ((k+1)*(3*n-k-2))/(n-k-2) + k/(n-k)  if n>k and n>k+2 else -999\n",
        "          AICF = n/math.log(sumRsc/n) * ((n*k)/(n-k-2) + 2) if n>k+2 else -999\n",
        "          RAIC = 2*sum(rhoError) + 2*k\n",
        "          \n",
        "          selectorValues = [AIC, AICc, KIC, KICc, AICF, KICc, RAIC]\n",
        "          for v in range (len(selectorValues)):\n",
        "            if selectorValues[v] < bestSelectorVal[v]:\n",
        "              bestSelectorVal[v] = selectorValues[v]\n",
        "              selectorError[v] = errorsEv[modelNum]\n",
        "          \n",
        "        else:\n",
        "          AIC, AICc, KIC, KICc, AKICc, AICF, RAIC = -999,-999,-999,-999,-999,-999,-999\n",
        "                \n",
        "        modelNum += 1\n",
        "        \n",
        "      #print(\"selectorError\", selectorError)\n",
        "      with open('/content/OutputMetrics/'+fname[-10:], 'a') as csvFile:\n",
        "        writer = csv.writer(csvFile, delimiter=';')\n",
        "\n",
        "        ss = 0  \n",
        "        modelSe = np.where(selectorError[0]==errorsEv) if selectorError[0]!=0 else [-999]\n",
        "        print(\"Model Selected\", int(modelSe[0]))\n",
        "        for mSel in selectorError:\n",
        "                    # participant, model, strategy, modelName, value\n",
        "          writer.writerow([part, modelSe[0], ss+1000, selStr[ss], mSel])  # participant,  selNum,  selName, value\n",
        "          ss += 1\n",
        "      csvFile.close()\n",
        "    else:\n",
        "      print(\"Not enough calibration points\")\n",
        "    print('___________________________________________')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-adaa67407f74>\"\u001b[0;36m, line \u001b[0;32m114\u001b[0m\n\u001b[0;31m    thr =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Bpo7BDMmXr11",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd OutputMetrics    && cat *.csv > AllM.csv\n",
        "!cd OutputTechniques && cat *.csv > AllT.csv"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}